{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90d51ca-7766-400c-9516-3085788d318b",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd33f3-993d-4731-820f-276359811a98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/Shamsvi/CMSE-830/main/CrimesOnWomenData.csv'\n",
    "\n",
    "# Attempting to load the CSV file from the raw GitHub link\n",
    "try:\n",
    "    data = pd.read_csv(url)\n",
    "    print(\"Data loaded successfully:\")\n",
    "    print(data.head())\n",
    "except pd.errors.ParserError as e:\n",
    "    print(\"ParserError occurred:\", e)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35b8cd8-efea-4d8d-b087-69ef09b42d84",
   "metadata": {},
   "source": [
    "### Data Collection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ff497-6c1c-4239-bca6-f6a0ada655ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "\n",
    "# Checking for duplicate rows\n",
    "duplicates = data.duplicated().sum()\n",
    "\n",
    "# Data types of the columns\n",
    "data_types = data.dtypes\n",
    "\n",
    "missing_values, duplicates, data_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054ffc4-bf54-4cab-b102-632728376c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Dropping unnecessary 'Unnamed: 0' column\n",
    "data_cleaned = data.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Step 2: Checking for missing values\n",
    "missing_values = data_cleaned.isnull().sum()\n",
    "\n",
    "# Step 3: Checking for duplicate rows and removing them\n",
    "duplicates = data_cleaned.duplicated().sum()\n",
    "data_cleaned = data_cleaned.drop_duplicates()\n",
    "\n",
    "# Step 4: Ensuring correct data types (converting Year to integer if necessary)\n",
    "data_cleaned['Year'] = data_cleaned['Year'].astype(int)\n",
    "\n",
    "# Display the results\n",
    "missing_values, duplicates, data_cleaned.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5faf176-8096-456d-bd5e-f1fd4eb2c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying One-Hot Encoding to the 'State' column\n",
    "data_encoded = pd.get_dummies(data_cleaned, columns=['State'], drop_first=True)\n",
    "\n",
    "# Display the first few rows to confirm the changes\n",
    "print(data_encoded.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc63c4-1a02-45d0-95b8-63d4b9b6f6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = data_encoded.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "# Visualize missing data using a heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(data_encoded.isnull(), cbar=False, cmap='viridis')\n",
    "plt.title(\"Missing Values Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c832557-907d-4e24-96fa-5db6a365a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of each crime type to see if there's any imbalance\n",
    "crime_counts = data_encoded[['Rape', 'K&A', 'DD', 'AoW', 'AoM', 'DV', 'WT']].sum()\n",
    "\n",
    "# Display the counts of each crime type\n",
    "crime_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea601a-d62a-443a-8d1a-e6f4475fecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "# Separate features (X) and target variables (y)\n",
    "X = data_encoded.drop(columns=['Rape', 'K&A', 'DD', 'AoW', 'AoM', 'DV', 'WT'])\n",
    "y_columns = ['Rape', 'K&A', 'DD', 'AoW', 'AoM', 'DV', 'WT']\n",
    "\n",
    "# Initialize a list to store the resampled targets\n",
    "y_resampled_list = []\n",
    "\n",
    "# Apply SMOTE for large datasets, RandomOverSampler for smaller ones\n",
    "for column in y_columns:\n",
    "    if data_encoded[column].value_counts().min() < 6:  # For small datasets\n",
    "        sampler = RandomOverSampler(random_state=42)\n",
    "    else:\n",
    "        sampler = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=5)  # Default SMOTE\n",
    "    \n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, data_encoded[column])\n",
    "    y_resampled_list.append(y_resampled)\n",
    "\n",
    "# Combine the resampled target columns back into a DataFrame\n",
    "y_resampled_final = pd.concat(y_resampled_list, axis=1)\n",
    "y_resampled_final.columns = y_columns  # Reassign column names\n",
    "\n",
    "# Combine the resampled features (X) with the resampled target columns (y)\n",
    "resampled_data = pd.concat([X_resampled, y_resampled_final], axis=1)\n",
    "\n",
    "# Display the first few rows of the resampled data\n",
    "print(resampled_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708419cb-a2c0-4cd7-b8e0-c2ba20834271",
   "metadata": {},
   "source": [
    "### Initial Data Analysis (IDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52f3e07-b8de-41b2-8854-c6ef2daecb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3081c0-0783-41ec-a557-b0f6ec9f8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates by merging the lowercase and uppercase columns\n",
    "# This example shows merging and summing duplicate columns for Assam\n",
    "\n",
    "data_encoded['State_Assam_combined'] = data_encoded[['State_ASSAM', 'State_Assam']].sum(axis=1)\n",
    "\n",
    "# After merging, drop the original columns\n",
    "data_encoded = data_encoded.drop(['State_ASSAM', 'State_Assam'], axis=1)\n",
    "\n",
    "# Repeat this process for all other duplicate columns, or loop over all columns systematically\n",
    "\n",
    "# Identify columns with duplicated state names\n",
    "import re\n",
    "\n",
    "# Create a function to merge duplicate state columns\n",
    "def merge_duplicate_states(df):\n",
    "    state_columns = [col for col in df.columns if re.match(r'State_', col)]\n",
    "    unique_states = set([re.sub(r'State_', '', col).lower() for col in state_columns])\n",
    "    \n",
    "    for state in unique_states:\n",
    "        matching_columns = [col for col in state_columns if re.sub(r'State_', '', col).lower() == state]\n",
    "        if len(matching_columns) > 1:\n",
    "            df[f'State_{state}_combined'] = df[matching_columns].sum(axis=1)\n",
    "            df.drop(matching_columns, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to your dataset\n",
    "data_encoded_clean = merge_duplicate_states(data_encoded)\n",
    "\n",
    "# Check the cleaned columns\n",
    "print(data_encoded_clean.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59375ca2-fbee-484f-8b52-9cc972306e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine all state columns into a single 'State' column\n",
    "state_columns = [col for col in data_encoded.columns if 'State_' in col]\n",
    "\n",
    "# Create a new 'State' column based on the non-null entries in the one-hot encoded columns\n",
    "data_encoded['State'] = data_encoded[state_columns].idxmax(axis=1).str.replace('State_', '')\n",
    "\n",
    "# Drop the old one-hot encoded state columns\n",
    "data_encoded.drop(columns=state_columns, inplace=True)\n",
    "\n",
    "# Display the new dataset to verify the 'State' column\n",
    "print(data_encoded[['State', 'Year', 'Rape', 'K&A', 'DD']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d91e60-e4fc-4ac1-b5ad-fe887121a792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796793ba-dfea-4708-86a3-7a6aeafd0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_statistics = data_encoded.describe()\n",
    "print(\"Summary Statistics:\\n\", summary_statistics)\n",
    "\n",
    "# Check for outliers using boxplots\n",
    "plt.figure(figsize=(10,6))\n",
    "data_encoded.drop(columns=['State', 'Year']).boxplot()\n",
    "plt.title(\"Boxplot of Crime Data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09841a37-4881-4a7d-9684-0d420ec01d36",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285a483-2436-424a-b9ae-92a54bfd4260",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Convert 'State' column to categorical if it's not already\n",
    "\n",
    "# Label Encoding for 'State' column\n",
    "label_encoder = LabelEncoder()\n",
    "data_encoded['State_encoded'] = label_encoder.fit_transform(data_encoded['State'])\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(data_encoded[['State', 'State_encoded']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef32017-330a-46e6-982a-3b1a3535c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each crime type\n",
    "data_encoded.drop(columns=['State', 'Year']).hist(figsize=(10, 10), bins=30)\n",
    "plt.suptitle(\"Distribution of Crimes Against Women\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc2f61-67cb-41d8-a734-69cf7e1bcd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "correlation_matrix = data_encoded.drop(columns=['State', 'Year']).corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap of Crime Types\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336796d-5c00-4824-9090-9d322bc02fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group by year and calculate the sum of numeric columns only\n",
    "yearly_trends = data_encoded.groupby('Year').sum(numeric_only=True)\n",
    "\n",
    "# Line chart for crime trends over the years\n",
    "plt.figure(figsize=(12, 8))\n",
    "yearly_trends.plot(kind='line')\n",
    "plt.title(\"Crime Trends Over the Years\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total Crimes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e903a-a5f9-498c-b095-d9b08dee9d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot total crimes over the years\n",
    "plt.figure(figsize=(10, 6))\n",
    "data.groupby('Year')['Rape'].sum().plot(kind='line', marker='o', color='r', label='Rape Cases')\n",
    "data.groupby('Year')['DV'].sum().plot(kind='line', marker='x', color='b', label='Domestic Violence Cases')\n",
    "plt.title('Crimes Against Women Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8440c-0d7e-4f81-aca9-43dc5e13f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the state names in the crime dataset to match the shapefile\n",
    "data['State'] = data['State'].replace({\n",
    "    'JAMMU & KASHMIR': 'Jammu and Kashmir',  \n",
    "    'Jammu & Kashmir': 'Jammu and Kashmir',  \n",
    "    'A & N Islands': 'Andaman and Nicobar',\n",
    "    'D & N HAVELI': 'Dadra and Nagar Haveli',\n",
    "    'D&N Haveli': 'Dadra and Nagar Haveli',\n",
    "    'DAMAN & DIU': 'Daman and Diu',\n",
    "    'Daman & Diu': 'Daman and Diu',\n",
    "    'Delhi UT': 'NCT of Delhi',\n",
    "    'ODISHA': 'Odisha', \n",
    "    'ANDHRA PRADESH': 'Andhra Pradesh',\n",
    "    'ARUNACHAL PRADESH': 'Arunachal Pradesh',\n",
    "    'ASSAM': 'Assam',\n",
    "    'BIHAR': 'Bihar',\n",
    "    'CHHATTISGARH': 'Chhattisgarh',\n",
    "    'GOA': 'Goa',\n",
    "    'GUJARAT': 'Gujarat',\n",
    "    'HARYANA': 'Haryana',\n",
    "    'HIMACHAL PRADESH': 'Himachal Pradesh',\n",
    "    'JHARKHAND': 'Jharkhand',\n",
    "    'KARNATAKA': 'Karnataka',\n",
    "    'KERALA': 'Kerala',\n",
    "    'MADHYA PRADESH': 'Madhya Pradesh',\n",
    "    'MAHARASHTRA': 'Maharashtra',\n",
    "    'MANIPUR': 'Manipur',\n",
    "    'MEGHALAYA': 'Meghalaya',\n",
    "    'MIZORAM': 'Mizoram',\n",
    "    'NAGALAND': 'Nagaland',\n",
    "    'PUNJAB': 'Punjab',\n",
    "    'RAJASTHAN': 'Rajasthan',\n",
    "    'SIKKIM': 'Sikkim',\n",
    "    'TAMIL NADU': 'Tamil Nadu',\n",
    "    'TRIPURA': 'Tripura',\n",
    "    'UTTAR PRADESH': 'Uttar Pradesh',\n",
    "    'UTTARAKHAND': 'Uttarakhand',\n",
    "    'WEST BENGAL': 'West Bengal',\n",
    "    'LAKSHADWEEP': 'Lakshadweep',\n",
    "    'PUDUCHERRY': 'Puducherry',\n",
    "})\n",
    "\n",
    "# Verify the updated state names\n",
    "print(data['State'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a296e7b-0453-474a-805f-d490abfa6630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# URL of the GitHub ZIP file\n",
    "url = 'https://github.com/Shamsvi/CMSE-830/archive/refs/heads/main.zip'\n",
    "\n",
    "# Download the file\n",
    "zip_path = 'gadm41_IND_shp.zip'\n",
    "with open(zip_path, 'wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "# Extract the downloaded ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall('gadm41_IND_shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581c508-1fed-4ba2-92ca-01aaf72d728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Download the ZIP file from GitHub (shapefile)\n",
    "url = 'https://github.com/Shamsvi/CMSE-830/archive/refs/heads/main.zip'\n",
    "\n",
    "# Download the ZIP file\n",
    "r = requests.get(url)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall(\"gadm41_IND_shp\")\n",
    "\n",
    "# Step 2: Load the shapefile using Geopandas\n",
    "shapefile_path = 'gadm41_IND_shp/CMSE-830-main/gadm41_IND_shp/gadm41_IND_1.shp'\n",
    "india_states = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Step 3: Load the crime dataset from the provided URL\n",
    "url = 'https://raw.githubusercontent.com/Shamsvi/CMSE-830/main/CrimesOnWomenData.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Step 4: Aggregate the crime data for all years\n",
    "crime_columns = ['Rape', 'K&A', 'DD', 'AoW', 'AoM', 'DV', 'WT']  # Adjust based on actual column names\n",
    "aggregated_data = data.groupby('State')[crime_columns].sum().reset_index()\n",
    "\n",
    "# Step 5: Merge the aggregated crime data with the shapefile\n",
    "merged_data = india_states.merge(aggregated_data, how='left', left_on='NAME_1', right_on='State')\n",
    "\n",
    "# Step 6: Create GeoJSON for Plotly\n",
    "geojson_data = merged_data.__geo_interface__\n",
    "\n",
    "# Step 7: Create an interactive map for one of the crimes (e.g., 'Rape')\n",
    "fig = px.choropleth_mapbox(\n",
    "    merged_data,\n",
    "    geojson=geojson_data,\n",
    "    locations='NAME_1',  # Column in merged_data for state names\n",
    "    featureidkey=\"properties.NAME_1\",  # Key for matching the GeoJSON\n",
    "    color='Rape',  # Change this to the column you want to visualize ('Rape', 'K&A', etc.)\n",
    "    color_continuous_scale=\"OrRd\",\n",
    "    mapbox_style=\"carto-positron\",\n",
    "    zoom=3.5,\n",
    "    center={\"lat\": 20.5937, \"lon\": 78.9629},  # Center the map on India\n",
    "    title=\"Crime Cases Across Indian States\"\n",
    ")\n",
    "\n",
    "# Step 8: Update layout for better aesthetics\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb689abe-6b7e-4526-bbdd-905b8122e94a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e6c18-b8fb-4b09-9357-fed95d8f5fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
